{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.blocks.convolutions import Convolution, ResidualUnit\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.networks.layers.simplelayers import SkipConnection\n",
    "from monai.utils import alias, export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric, compute_meandice\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import h5py\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = []\n",
    "end = []\n",
    "for i in range(625):\n",
    "    if i >= 0 and i <= 24:\n",
    "        if i == 0:\n",
    "            start.append(0)\n",
    "            end.append(1)\n",
    "            start.append(0)\n",
    "            end.append(50)\n",
    "        elif i == 24:\n",
    "            start.append(24)\n",
    "            end.append(23)\n",
    "            start.append(24)\n",
    "            end.append(49)\n",
    "        else:\n",
    "            start.append(i)\n",
    "            end.append(i-1)\n",
    "            start.append(i)\n",
    "            end.append(i+1)\n",
    "            start.append(i)\n",
    "            end.append(i+25)\n",
    "    elif (i >= 600 and i <= 624):\n",
    "        if i == 600:\n",
    "            start.append(600)\n",
    "            end.append(575)\n",
    "            start.append(600)\n",
    "            end.append(601)\n",
    "        elif i == 624:\n",
    "            start.append(600)\n",
    "            end.append(575)\n",
    "            start.append(600)\n",
    "            end.append(601)\n",
    "        else:\n",
    "            start.append(i)\n",
    "            end.append(i-1)\n",
    "            start.append(i)\n",
    "            end.append(i+1)\n",
    "            start.append(i)\n",
    "            end.append(i-25)\n",
    "    elif i % 25 == 0:\n",
    "        start.append(i)\n",
    "        end.append(i+1)\n",
    "        start.append(i)\n",
    "        end.append(i+25)\n",
    "        start.append(i)\n",
    "        end.append(i-25)\n",
    "    elif i % 25 == 24:\n",
    "        start.append(i)\n",
    "        end.append(i-1)\n",
    "        start.append(i)\n",
    "        end.append(i+25)\n",
    "        start.append(i)\n",
    "        end.append(i-25)     \n",
    "    else:\n",
    "        start.append(i)\n",
    "        end.append(i-1)\n",
    "        start.append(i)\n",
    "        end.append(i+25)\n",
    "        start.append(i)\n",
    "        end.append(i-25)  \n",
    "        start.append(i)\n",
    "        end.append(i+1)\n",
    "edges = torch.tensor([start, end], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = './'\n",
    "data_path = [base + 'train/' + x for x in os.listdir(base + 'train/')]\n",
    "names = [x.split('/')[-1].split('_')[0] for x in data_path]\n",
    "counts = pd.Series(names).value_counts().to_dict()\n",
    "templete =  './train/{x}_{y}.h5'\n",
    "train_paths = []\n",
    "for name in counts.keys():\n",
    "    count = counts[name]\n",
    "    for i in range(1, count-2):\n",
    "        temp = []\n",
    "        for j in range(i-1, i+2):\n",
    "            temp.append(templete.format(x=name, y=j))\n",
    "        train_paths.append(temp)\n",
    "\n",
    "base = './'\n",
    "data_path = [base + 'val/' + x for x in os.listdir(base + 'val/')]\n",
    "names = [x.split('/')[-1].split('_')[0] for x in data_path]\n",
    "counts = pd.Series(names).value_counts().to_dict()\n",
    "templete =  './val/{x}_{y}.h5'\n",
    "val_paths = []\n",
    "for name in counts.keys():\n",
    "    count = counts[name]\n",
    "    for i in range(1, count-2):\n",
    "        temp = []\n",
    "        for j in range(i-1, i+2):\n",
    "            temp.append(templete.format(x=name, y=j))\n",
    "        val_paths.append(temp)\n",
    "\n",
    "class segmentation(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, aug=False, train=True):\n",
    "        self.paths = paths\n",
    "        self.train = train\n",
    "        self.aug = aug\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        paths = self.paths[idx]\n",
    "        xs = []\n",
    "        f = h5py.File(paths[1], 'r')\n",
    "        y = f['gt'][:]\n",
    "        y = torch.from_numpy(y)\n",
    "        y = y.float().view(1, 400, 400)\n",
    "        f.close()\n",
    "        for path in paths:\n",
    "            f = h5py.File(path, 'r')\n",
    "            x = f['ct'][:].astype(np.float)\n",
    "            x = torch.from_numpy(x)\n",
    "            x = x.float().view(1, 400, 400)\n",
    "            xs.append(x)  \n",
    "            f.close()\n",
    "        x = torch.cat(xs, dim=0)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "def one_hot(labels: torch.Tensor, num_classes: int, dtype: torch.dtype = torch.float, dim: int = 1) -> torch.Tensor:\n",
    "    # if `dim` is bigger, add singleton dim at the end\n",
    "    if labels.ndim < dim + 1:\n",
    "        shape = list(labels.shape) + [1] * (dim + 1 - len(labels.shape))\n",
    "        labels = torch.reshape(labels, shape)\n",
    "\n",
    "    sh = list(labels.shape)\n",
    "\n",
    "    if sh[dim] != 1:\n",
    "        raise AssertionError(\"labels should have a channel with length equal to one.\")\n",
    "\n",
    "    sh[dim] = num_classes\n",
    "\n",
    "    o = torch.zeros(size=sh, dtype=dtype, device=labels.device)\n",
    "    labels = o.scatter_(dim=dim, index=labels.long(), value=1)\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_paths[0]\n",
    "xs = []\n",
    "f = h5py.File(test[1], 'r')\n",
    "y_test = f['gt'][:]\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = y_test.float().view(1, 400, 400)\n",
    "f.close()\n",
    "for path in test:\n",
    "    f = h5py.File(path, 'r')\n",
    "    x_test = f['ct'][:].astype(np.float)\n",
    "    x_test = torch.from_numpy(x_test)\n",
    "    x_test = x_test.float().view(1, 400, 400)\n",
    "    xs.append(x_test)  \n",
    "    f.close()\n",
    "x_test = torch.cat(xs, dim=0)\n",
    "x_test = x_test.view(1, 3, 400, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        channels: Sequence[int],\n",
    "        strides: Sequence[int],\n",
    "        kernel_size: Union[Sequence[int], int] = 3,\n",
    "        up_kernel_size: Union[Sequence[int], int] = 3,\n",
    "        num_res_units: int = 0,\n",
    "        act: Union[Tuple, str] = Act.PRELU,\n",
    "        norm: Union[Tuple, str] = Norm.INSTANCE,\n",
    "        dropout=0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        delta = len(strides) - (len(channels) - 1)\n",
    "        self.dimensions = dimensions\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_kernel_size = up_kernel_size\n",
    "        self.num_res_units = num_res_units\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.dropout = dropout\n",
    "        self.downs = []\n",
    "        self.ups = []\n",
    "        self.downs = []\n",
    "        \n",
    "        \n",
    "        #graph sage conv\n",
    "        self.sageconv1 = SAGEConv(in_channels = 256, out_channels = 256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sageconv2 = SAGEConv(in_channels = 256, out_channels = 256)\n",
    "        self.linear =  nn.Linear(512, 256)\n",
    "        def _create_block(\n",
    "            inc: int, outc: int, channels: Sequence[int], strides: Sequence[int], is_top: bool\n",
    "        ) -> nn.Sequential:\n",
    "            c = channels[0]\n",
    "            s = strides[0]\n",
    "\n",
    "            subblock: nn.Module\n",
    "\n",
    "            if len(channels) > 2:\n",
    "                _create_block(c, c, channels[1:], strides[1:], False)\n",
    "                upc = c * 2\n",
    "            else:\n",
    "                self.subblock = self._get_bottom_layer(c, channels[1])\n",
    "                upc = c + channels[1]\n",
    "\n",
    "            self.downs.append(self._get_down_layer(inc, c, s, is_top))\n",
    "            self.ups.append(self._get_up_layer(upc, outc, s, is_top))\n",
    "        \n",
    "        _create_block(in_channels, out_channels, self.channels, self.strides, True)\n",
    "        print(len(self.ups), len(self.downs))\n",
    "        self.up1, self.up2, self.up3, self.up4 = self.ups\n",
    "        del self.ups\n",
    "        self.down1, self.down2, self.down3, self.down4 = self.downs\n",
    "        del self.downs\n",
    "\n",
    "    def _get_down_layer(self, in_channels: int, out_channels: int, strides: int, is_top: bool) -> nn.Module:\n",
    "        if self.num_res_units > 0:\n",
    "            return ResidualUnit(\n",
    "                self.dimensions,\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                strides=strides,\n",
    "                kernel_size=self.kernel_size,\n",
    "                subunits=self.num_res_units,\n",
    "                act=self.act,\n",
    "                norm=self.norm,\n",
    "                dropout=self.dropout,\n",
    "            )\n",
    "        return Convolution(\n",
    "            self.dimensions,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            strides=strides,\n",
    "            kernel_size=self.kernel_size,\n",
    "            act=self.act,\n",
    "            norm=self.norm,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "    def _get_bottom_layer(self, in_channels: int, out_channels: int) -> nn.Module:\n",
    "        return self._get_down_layer(in_channels, out_channels, 1, False)\n",
    "\n",
    "    def _get_up_layer(self, in_channels: int, out_channels: int, strides: int, is_top: bool) -> nn.Module:\n",
    "        conv: Union[Convolution, nn.Sequential]\n",
    "\n",
    "        conv = Convolution(\n",
    "            self.dimensions,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            strides=strides,\n",
    "            kernel_size=self.up_kernel_size,\n",
    "            act=self.act,\n",
    "            norm=self.norm,\n",
    "            dropout=self.dropout,\n",
    "            conv_only=is_top and self.num_res_units == 0,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "\n",
    "        if self.num_res_units > 0:\n",
    "            ru = ResidualUnit(\n",
    "                self.dimensions,\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                strides=1,\n",
    "                kernel_size=self.kernel_size,\n",
    "                subunits=1,\n",
    "                act=self.act,\n",
    "                norm=self.norm,\n",
    "                dropout=self.dropout,\n",
    "                last_conv_only=is_top,\n",
    "            )\n",
    "            conv = nn.Sequential(conv, ru)\n",
    "\n",
    "        return conv\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device, edges) -> torch.Tensor:\n",
    "        edges = edges.to(device)\n",
    "        xs = []        \n",
    "        for m in [self.down4, self.down3, self.down2, self.down1]:\n",
    "            x = m(x)\n",
    "            #print(x.shape)\n",
    "            xs.append(x)\n",
    "        \n",
    "        x = self.subblock(x)   \n",
    "        #print(x.shape)\n",
    "        graph_x = x.view(x.shape[0], 256, -1).permute(0, 2, 1)\n",
    "        #print(x.shape)\n",
    "        graph_x = self.sageconv1(x=graph_x, edge_index=edges)\n",
    "        graph_x = self.relu(graph_x)\n",
    "        graph_x = self.sageconv2(x=graph_x, edge_index=edges)\n",
    "        graph_x = self.relu(graph_x)\n",
    "        graph_x = graph_x.permute(0, 2, 1).view(graph_x.shape[0], 256, 25, 25).float()\n",
    "        \n",
    "        x = torch.cat([x, graph_x], dim=1).permute(0, 2, 3, 1)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #print(x.shape)\n",
    "        for m, cat in zip([self.up1, self.up2, self.up3, self.up4], xs[::-1]):\n",
    "            x = torch.cat([cat, x], dim=1)\n",
    "            x = m(x)\n",
    "            #print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,model,train_set,test_set,opts):\n",
    "        self.model = model  # neural net\n",
    "        # device agnostic code snippet\n",
    "        self.device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.epochs = opts['epochs']\n",
    "        self.scaler = GradScaler()\n",
    "        self.optim = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.001)\n",
    "        self.criterion = DiceLoss(to_onehot_y=True, softmax=True, squared_pred=False)                     # loss function\n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                                        batch_size=opts['batch_size'],\n",
    "                                                        shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                                       batch_size=opts['batch_size'],\n",
    "                                                       shuffle=False)\n",
    "        #self.tb = SummaryWriter(log_dir='./runs/unet_2/')\n",
    "        self.best_loss = 0\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train() #put model in training mode\n",
    "            self.tr_loss = []\n",
    "            for i, (data,labels) in tqdm(enumerate(self.train_loader),\n",
    "                                                   total = len(self.train_loader)):\n",
    "                data, labels = data.to(self.device),labels.to(self.device)\n",
    "                self.optimizer.zero_grad()  \n",
    "                outputs = self.model(data, self.device, edges)   \n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()                        \n",
    "                self.optimizer.step()                  \n",
    "                self.tr_loss.append(loss.item())\n",
    "                #self.tb.add_scalar(\"Train Loss\", np.mean(self.tr_loss), epoch)\n",
    "            \n",
    "            self.test(epoch) # run through the validation set\n",
    "        self.tb.close()\n",
    "            \n",
    "    def test(self,epoch):\n",
    "            \n",
    "            self.model.eval()    # puts model in eval mode - not necessary for this demo but good to know\n",
    "            self.test_loss = []\n",
    "            self.test_dice = []\n",
    "            self.test_acc = []\n",
    "            \n",
    "            for i, (data, labels) in enumerate(self.test_loader):\n",
    "                \n",
    "                data, labels = data.to(self.device),labels.to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(data, self.device, edges)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                self.test_loss.append(loss.item())\n",
    "                outputs = torch.nn.functional.softmax(outputs, 1)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                predicted = predicted.view(-1, 1, 400, 400)\n",
    "                temp_dice = compute_meandice(one_hot(predicted, 2), one_hot(labels, 2), include_background=False).detach().cpu().numpy()\n",
    "                if np.nanmean(temp_dice) == np.nanmean(temp_dice):\n",
    "                    self.test_dice.append(np.nanmean(temp_dice))\n",
    "                self.test_acc.append((predicted == labels).sum().item() / (predicted.size(0)*400*400))\n",
    "               \n",
    "            print('epoch: {}, train loss: {}, test loss: {}'.format( \n",
    "                  epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss)))\n",
    "            print('epoch: {}, test dice: {}, test acc: {}'.format( \n",
    "                  epoch+1, np.nanmean(self.test_dice), np.mean(self.test_acc)))           \n",
    "            #self.tb.add_scalar(\"Val Loss\", np.mean(self.test_loss), epoch)\n",
    "            #self.tb.add_scalar(\"Val dice\", np.nanmean(self.test_dice), epoch)\n",
    "            #self.tb.add_scalar(\"Val acc\", np.mean(self.test_acc), epoch)\n",
    "            if np.nanmean(self.test_dice) > self.best_loss:\n",
    "                self.best_loss = np.nanmean(self.test_dice)\n",
    "                #torch.save(self.model, './model_weights/best_unet_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n"
     ]
    }
   ],
   "source": [
    "model = UNet_GNN(\n",
    "    dimensions=2,\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(x_test, torch.device('cpu'), edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199c5497b0ac4ad197bc9c856025e989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1, train loss: 0.6082643393486266, test loss: 0.596521932631731\n",
      "epoch: 1, test dice: 0.12290536612272263, test acc: 0.6805059387207032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c27f9caf7e4db0a0a24607eae3358a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b4323617e391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-eaf053fd7c51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#put model in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             for i, (data,labels) in tqdm(enumerate(self.train_loader),\n\u001b[0m\u001b[1;32m     27\u001b[0m                                                    total = len(self.train_loader)):\n\u001b[1;32m     28\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xinrui/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xinrui/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xinrui/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xinrui/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xinrui/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xinrui/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d52e900f11c3>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ct'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_set, val_set = segmentation(train_paths[:2000]), segmentation(val_paths[:1000], train=False)\n",
    "\n",
    "opts = {\n",
    "    'lr': 5e-4,\n",
    "    'epochs': 40,\n",
    "    'batch_size': 32\n",
    "}\n",
    "train = Trainer(model, train_set, val_set, opts)\n",
    "train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinrui",
   "language": "python",
   "name": "xinrui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
