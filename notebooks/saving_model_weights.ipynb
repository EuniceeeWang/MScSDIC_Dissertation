{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.blocks.convolutions import Convolution, ResidualUnit\n",
    "from monai.networks.layers.factories import Act, Norm\n",
    "from monai.networks.layers.simplelayers import SkipConnection\n",
    "from monai.utils import alias, export\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric, compute_meandice\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import h5py\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from monai.networks.nets import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        channels: Sequence[int],\n",
    "        strides: Sequence[int],\n",
    "        kernel_size: Union[Sequence[int], int] = 3,\n",
    "        up_kernel_size: Union[Sequence[int], int] = 3,\n",
    "        num_res_units: int = 0,\n",
    "        act: Union[Tuple, str] = Act.PRELU,\n",
    "        norm: Union[Tuple, str] = Norm.INSTANCE,\n",
    "        dropout=0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        delta = len(strides) - (len(channels) - 1)\n",
    "        self.dimensions = dimensions\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_kernel_size = up_kernel_size\n",
    "        self.num_res_units = num_res_units\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.dropout = dropout\n",
    "        self.downs = []\n",
    "        self.ups = []\n",
    "        self.downs = []\n",
    "        \n",
    "        \n",
    "        #graph sage conv\n",
    "        self.sageconv1 = SAGEConv(in_channels = 256, out_channels = 256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sageconv2 = SAGEConv(in_channels = 256, out_channels = 256)\n",
    "        def _create_block(\n",
    "            inc: int, outc: int, channels: Sequence[int], strides: Sequence[int], is_top: bool\n",
    "        ) -> nn.Sequential:\n",
    "            c = channels[0]\n",
    "            s = strides[0]\n",
    "\n",
    "            subblock: nn.Module\n",
    "\n",
    "            if len(channels) > 2:\n",
    "                _create_block(c, c, channels[1:], strides[1:], False)\n",
    "                upc = c * 2\n",
    "            else:\n",
    "                self.subblock = self._get_bottom_layer(c, channels[1])\n",
    "                upc = c + channels[1]\n",
    "\n",
    "            self.downs.append(self._get_down_layer(inc, c, s, is_top))\n",
    "            self.ups.append(self._get_up_layer(upc, outc, s, is_top))\n",
    "        \n",
    "        _create_block(in_channels, out_channels, self.channels, self.strides, True)\n",
    "        self.up1, self.up2, self.up3, self.up4 = self.ups\n",
    "        del self.ups\n",
    "        self.down1, self.down2, self.down3, self.down4 = self.downs\n",
    "        del self.downs\n",
    "\n",
    "    def _get_down_layer(self, in_channels: int, out_channels: int, strides: int, is_top: bool) -> nn.Module:\n",
    "        if self.num_res_units > 0:\n",
    "            return ResidualUnit(\n",
    "                self.dimensions,\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                strides=strides,\n",
    "                kernel_size=self.kernel_size,\n",
    "                subunits=self.num_res_units,\n",
    "                act=self.act,\n",
    "                norm=self.norm,\n",
    "                dropout=self.dropout,\n",
    "            )\n",
    "        return Convolution(\n",
    "            self.dimensions,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            strides=strides,\n",
    "            kernel_size=self.kernel_size,\n",
    "            act=self.act,\n",
    "            norm=self.norm,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "    def _get_bottom_layer(self, in_channels: int, out_channels: int) -> nn.Module:\n",
    "        return self._get_down_layer(in_channels, out_channels, 1, False)\n",
    "\n",
    "    def _get_up_layer(self, in_channels: int, out_channels: int, strides: int, is_top: bool) -> nn.Module:\n",
    "        conv: Union[Convolution, nn.Sequential]\n",
    "\n",
    "        conv = Convolution(\n",
    "            self.dimensions,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            strides=strides,\n",
    "            kernel_size=self.up_kernel_size,\n",
    "            act=self.act,\n",
    "            norm=self.norm,\n",
    "            dropout=self.dropout,\n",
    "            conv_only=is_top and self.num_res_units == 0,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "\n",
    "        if self.num_res_units > 0:\n",
    "            ru = ResidualUnit(\n",
    "                self.dimensions,\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                strides=1,\n",
    "                kernel_size=self.kernel_size,\n",
    "                subunits=1,\n",
    "                act=self.act,\n",
    "                norm=self.norm,\n",
    "                dropout=self.dropout,\n",
    "                last_conv_only=is_top,\n",
    "            )\n",
    "            conv = nn.Sequential(conv, ru)\n",
    "\n",
    "        return conv\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device, edges) -> torch.Tensor:\n",
    "        edges = edges.to(device)\n",
    "        xs = []        \n",
    "        for m in [self.down4, self.down3, self.down2, self.down1]:\n",
    "            x = m(x)\n",
    "            #print(x.shape)\n",
    "            xs.append(x)\n",
    "        \n",
    "        x = self.subblock(x)   \n",
    "        #print(x.shape)\n",
    "        x = x.view(x.shape[0], 256, -1).permute(0, 2, 1)\n",
    "        #print(x.shape)\n",
    "        x = self.sageconv1(x=x, edge_index=edges)\n",
    "        x = self.relu(x)\n",
    "        x = self.sageconv2(x=x, edge_index=edges)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1).view(x.shape[0], 256, 25, 25).float()\n",
    "        #print(x.shape)\n",
    "        for m, cat in zip([self.up1, self.up2, self.up3, self.up4], xs[::-1]):\n",
    "            x = torch.cat([cat, x], dim=1)\n",
    "            x = m(x)\n",
    "            #print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = torch.load('./model_weights/best_unet.pt')\n",
    "sageconv = torch.load('./model_weights/best_unet_sage_conv_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), './model_weights/best_unet_weight.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sageconv.state_dict(), './model_weights/best_sage_conv_weight.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        channels: Sequence[int],\n",
    "        strides: Sequence[int],\n",
    "        kernel_size: Union[Sequence[int], int] = 3,\n",
    "        up_kernel_size: Union[Sequence[int], int] = 3,\n",
    "        num_res_units: int = 0,\n",
    "        act: Union[Tuple, str] = Act.PRELU,\n",
    "        norm: Union[Tuple, str] = Norm.INSTANCE,\n",
    "        dropout=0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        delta = len(strides) - (len(channels) - 1)\n",
    "        self.dimensions = dimensions\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_kernel_size = up_kernel_size\n",
    "        self.num_res_units = num_res_units\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.dropout = dropout\n",
    "        self.downs = []\n",
    "        self.ups = []\n",
    "        self.downs = []\n",
    "        \n",
    "        \n",
    "        #graph sage conv\n",
    "        self.sageconv1 = SAGEConv(in_channels = 256, out_channels = 256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sageconv2 = SAGEConv(in_channels = 256, out_channels = 256)\n",
    "        self.linear =  nn.Linear(512, 256)\n",
    "        def _create_block(\n",
    "            inc: int, outc: int, channels: Sequence[int], strides: Sequence[int], is_top: bool\n",
    "        ) -> nn.Sequential:\n",
    "            c = channels[0]\n",
    "            s = strides[0]\n",
    "\n",
    "            subblock: nn.Module\n",
    "\n",
    "            if len(channels) > 2:\n",
    "                _create_block(c, c, channels[1:], strides[1:], False)\n",
    "                upc = c * 2\n",
    "            else:\n",
    "                self.subblock = self._get_bottom_layer(c, channels[1])\n",
    "                upc = c + channels[1]\n",
    "\n",
    "            self.downs.append(self._get_down_layer(inc, c, s, is_top))\n",
    "            self.ups.append(self._get_up_layer(upc, outc, s, is_top))\n",
    "        \n",
    "        _create_block(in_channels, out_channels, self.channels, self.strides, True)\n",
    "        print(len(self.ups), len(self.downs))\n",
    "        self.up1, self.up2, self.up3, self.up4 = self.ups\n",
    "        del self.ups\n",
    "        self.down1, self.down2, self.down3, self.down4 = self.downs\n",
    "        del self.downs\n",
    "\n",
    "    def _get_down_layer(self, in_channels: int, out_channels: int, strides: int, is_top: bool) -> nn.Module:\n",
    "        if self.num_res_units > 0:\n",
    "            return ResidualUnit(\n",
    "                self.dimensions,\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                strides=strides,\n",
    "                kernel_size=self.kernel_size,\n",
    "                subunits=self.num_res_units,\n",
    "                act=self.act,\n",
    "                norm=self.norm,\n",
    "                dropout=self.dropout,\n",
    "            )\n",
    "        return Convolution(\n",
    "            self.dimensions,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            strides=strides,\n",
    "            kernel_size=self.kernel_size,\n",
    "            act=self.act,\n",
    "            norm=self.norm,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "    def _get_bottom_layer(self, in_channels: int, out_channels: int) -> nn.Module:\n",
    "        return self._get_down_layer(in_channels, out_channels, 1, False)\n",
    "\n",
    "    def _get_up_layer(self, in_channels: int, out_channels: int, strides: int, is_top: bool) -> nn.Module:\n",
    "        conv: Union[Convolution, nn.Sequential]\n",
    "\n",
    "        conv = Convolution(\n",
    "            self.dimensions,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            strides=strides,\n",
    "            kernel_size=self.up_kernel_size,\n",
    "            act=self.act,\n",
    "            norm=self.norm,\n",
    "            dropout=self.dropout,\n",
    "            conv_only=is_top and self.num_res_units == 0,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "\n",
    "        if self.num_res_units > 0:\n",
    "            ru = ResidualUnit(\n",
    "                self.dimensions,\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                strides=1,\n",
    "                kernel_size=self.kernel_size,\n",
    "                subunits=1,\n",
    "                act=self.act,\n",
    "                norm=self.norm,\n",
    "                dropout=self.dropout,\n",
    "                last_conv_only=is_top,\n",
    "            )\n",
    "            conv = nn.Sequential(conv, ru)\n",
    "\n",
    "        return conv\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device, edges) -> torch.Tensor:\n",
    "        edges = edges.to(device)\n",
    "        xs = []        \n",
    "        for m in [self.down4, self.down3, self.down2, self.down1]:\n",
    "            x = m(x)\n",
    "            #print(x.shape)\n",
    "            xs.append(x)\n",
    "        \n",
    "        x = self.subblock(x)   \n",
    "        #print(x.shape)\n",
    "        graph_x = x.view(x.shape[0], 256, -1).permute(0, 2, 1)\n",
    "        #print(x.shape)\n",
    "        graph_x = self.sageconv1(x=graph_x, edge_index=edges)\n",
    "        graph_x = self.relu(graph_x)\n",
    "        graph_x = self.sageconv2(x=graph_x, edge_index=edges)\n",
    "        graph_x = self.relu(graph_x)\n",
    "        graph_x = graph_x.permute(0, 2, 1).view(graph_x.shape[0], 256, 25, 25).float()\n",
    "        \n",
    "        x = torch.cat([x, graph_x], dim=1).permute(0, 2, 3, 1)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        #print(x.shape)\n",
    "        for m, cat in zip([self.up1, self.up2, self.up3, self.up4], xs[::-1]):\n",
    "            x = torch.cat([cat, x], dim=1)\n",
    "            x = m(x)\n",
    "            #print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sageconv_linear = torch.load('model_weights/best_unet_sageconv_linear.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sageconv_linear.state_dict(), './model_weights/best_sage_conv_linear_weight.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinrui",
   "language": "python",
   "name": "xinrui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
